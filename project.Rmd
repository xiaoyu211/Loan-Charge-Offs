#Step 1: Clean data
```{r}
# data <- read.csv("~/Desktop/LoanStats3c.csv")
# data <- data[data$X != "-1",]
# colnames(data)[10] <- "status"
# data <- data[complete.cases(data),]
# saveRDS(data,"~/Desktop/4201data.Rds")
```

## import dataset, remove NAs cases.

#Step 2: Visualization and Variable selection.
```{r}

```

#Step 3: Split training and testing dataset
```{r}
data <- readRDS("~/Desktop/all_data/4201data.Rds")
n <- sample(1:81613,1000)
data <- data[n,]

#select some variables
data2 <- data[,c(1,3,4,6,7,8,9,13:24)];data2

#label
label <- data.frame(label = as.factor(data[,10]))

i <- sample(1:nrow(data2),nrow(data2)*0.8) 
train_data <- data2[i,] 
test_data <- data2[-i,]

train_label <- label[i,]
test_label <-  label[-i,]
```

# Step 3: First method: GLM
```{r}
data=read.csv('LoanStats3c.csv')
data=data[data[,10]!='-1',-11]
data=data[complete.cases(data),]
status=1-data[,10]
attach(data)
full=glm(status~.,data=data[,c(-10)],family=binomial())
null=glm(status~1,data=data[,c(-10)],family=binomial())

pairs(data[,c(-2,-5,-6,-7,-9,-10,-11)][sample(10000),])
corre=cor(data[,c(-2,-5,-6,-7,-9,-10,-11)])
library(lattice)
levelplot(corre,col.regions = terrain.colors(20))
#check verification
par(mfrow=c(1, 3))
hist(status[verification_status=='Verified'])
hist(status[verification_status=='Source Verified'])
hist(status[verification_status=='Not Verified'])
summary(glm(status~verification_status,family=binomial))
#relevel verification
levels(data[,9])=c("Not Verified","Verified","Verified")
#
data[1,-c(-5,-6,-7,-9,-10,-11,-19)]
l2=glm(status~(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+(tot_cur_bal*avg_cur_bal):mort_acc+int_rate
+dti+delinq_2yrs*pct_tl_nvr_dlq*num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1),data=data[,-10],family=binomial)
summary(l2)
#only adjust numerical terms
l3=glm(status~(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1),data=data[,-10],family=binomial)
summary(l3)
anova(l3,l2)
1-pchisq(6.695,5)
#term interaction
l4=glm(status~(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1)
+(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75):term
,data=data[,-10],family=binomial)
anova(l3,l4)
1-pchisq(46.55,48)
#grade interaction
l6=glm(status~loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1
+(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75):grade
,data=data[,-10],family=binomial)
anova(l3,l6)
1-pchisq(367.87,287)
summary(l6)
levels(data[,5])=c('N','N','N','N','N','N','G')
l5=glm(status~loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1
+(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75):grade
,data=data[,-10],family=binomial)
anova(l5,l6)
1-pchisq(456.47,245)
#
l7=glm(status~loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1
+(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75):emp_length
,data=data[,-10],family=binomial)
anova(l3,l7)
1-pchisq(659.18,517)
summary(l7)
#try combine every two adjacent levels
levels(data[,6])
levels(data[,6])=c('< 7 years','< 7 years','>= 7 years','< 7 years',
'< 7 years','< 7 years','< 7 years','< 7 years','>= 7 years','>= 7 years',
'>= 7 years','< 7 years')

l8=glm(status~loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1
+(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75):emp_length
,data=data[,-10],family=binomial)
anova(l8,l7)
1-pchisq(762.43,437)
#
l9=glm(status~loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1
+(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75):home_ownership
,data=data[,-10],family=binomial)
anova(l3,l9)
1-pchisq(2.9649,87)

l11=glm(status~loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1
+(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75):verification_status
,data=data[,-10],family=binomial)
anova(l3,l11)
1-pchisq(14.573,89)

l13=glm(status~loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1
+(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75):purpose
,data=data[,-10],family=binomial)
anova(l3,l13)
1-pchisq(517.23,526)

###only emp_length and grade interactions are significant
ll1=glm(status~loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75
+term+grade+emp_length+home_ownership+verification_status+purpose-1
+(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd+inq_last_6mths
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75):grade
+(loan_amnt*installment*annual_inc*tot_cur_bal*avg_cur_bal+mort_acc
+avg_cur_bal:mort_acc+tot_cur_bal:mort_acc+int_rate
+dti+delinq_2yrs+pct_tl_nvr_dlq+num_accts_ever_120_pd
+open_acc*total_acc*num_bc_sats+percent_bc_gt_75):emp_length
,data=data[,-10],family=binomial)
anova(ll1,l8)
1-pchisq()

step(null,scope=list(lower=null,upper=ll1),direction='forward',steps=20)
final=glm(formula = status ~ int_rate + dti + tot_cur_bal + emp_length + 
    loan_amnt + total_acc + annual_inc + home_ownership + delinq_2yrs + 
    open_acc + percent_bc_gt_75 + term + inq_last_6mths + purpose + 
    verification_status + pct_tl_nvr_dlq + installment + loan_amnt:annual_inc + 
    tot_cur_bal:annual_inc + loan_amnt:installment, family = binomial(), 
    data = data[, c(-10)])
mean((predict(final)>0)==status)
summary(final)
#delete home_ownership
final2=glm(formula = status ~ int_rate + dti + tot_cur_bal + emp_length + 
    loan_amnt + total_acc + annual_inc  + delinq_2yrs + 
    open_acc + percent_bc_gt_75 + term + inq_last_6mths + purpose + 
    verification_status + pct_tl_nvr_dlq + installment + loan_amnt:annual_inc + 
    tot_cur_bal:annual_inc + loan_amnt:installment, family = binomial(), 
    data = data[, c(-10)])
summary(final2)
mean((predict(final2)>0)==status)
#remove purpose and loan_amnt
final3=glm(formula = status ~ int_rate + dti + tot_cur_bal + emp_length + 
    total_acc + annual_inc  + delinq_2yrs + 
    open_acc + percent_bc_gt_75 + term + inq_last_6mths +  
    verification_status + pct_tl_nvr_dlq + installment + loan_amnt:annual_inc + 
    tot_cur_bal:annual_inc + loan_amnt:installment, family = binomial(), 
    data = data[, c(-10)])
summary(final3)
plot(1:12,c(-0.2269,0,0.09417,0.1319,0.1354,0.1493,0.07409,0.1114,0.1535,0.1077,
0.09613,0.2298),ylab='Employment Length coeffients',xlab='Employment Length')

par(mfrow=c(1,2))
boxplot(predict(final3,type='response')[status==1])
boxplot(predict(final3,type='response')[status==0])
p=0.8
v=0
for (i in 1:100){
s=mean((predict(final3,type='response')>i/100)==status)
if (s>p){p=s;v=i/100}}
mean((predict(final3,type='response')>v)==status)
```

# Step 4: Second method: Random forest and model selection.
```{r}
library(randomForest)

# Cross-Valdidation for feature selection
# result <- rfcv(train_data, train_label, cv.fold = 5, scale="log", step = 0.5)
# with(result, plot(n.var, error.cv, log="x", type="o", lwd=2))

# Tune randomForest for the optimal mtry parameter
# result2 <- tuneRF(train_data, train_label, mtryStart = 10, ntreeTry = 50, stepFactor = 2, improve = 0.01,trace = TRUE, plot=TRUE, doBest=FALSE)

# output3 <- randomForest(train_data,train_label, mtry = 10, ntree = 500 , importance = T)
# output3
# varImpPlot(output3)
# 
# #prediction
# pred3 <- predict(output3,test_data)
# rate3 <- mean(pred3 == test_label);rate3

#rebuild model
newtrain <- train_data[,c(-4,-5,-7,-9,-10,-15,-16)]
newtest <- test_data[,c(-4,-5,-7,-9,-10,-15,-16)]

result2 <- tuneRF(train_data, train_label, mtryStart = 10, ntreeTry = 50, stepFactor = 2, improve = 0.01,trace = TRUE, plot=TRUE, doBest=FALSE)

output31 <- randomForest(newtrain,train_label, mtry = 3, ntree = 500, importance = T)
pred31 <- predict(output31,newtest)
RF_rate <- mean(pred31 == test_label);RF_rate
```

### 5 fold Cross-Valdidation for feature selection.
### Tune randomForest for the optimal mtry parameter
### Train the model with optimal feature and parameter.

# 3rd Method: SVM with RBF kernel
```{r}
#Normalizing dataset
library(clusterSim)
library(cluster)
library(MASS)
library(e1071)

#normalize data set
newtrain <- data.Normalization(newtrain, type ="n1",normalization="column")
newtest <- data.Normalization(newtest, type ="n1",normalization="column")

#parameters selection by 10-fold cv
# output2 <- tune(svm,newtrain,train_label,kernel = "radial",
#                ranges = list(cost = c(0.1, 1, 10, 100, 1000),
#                               gamma =c(0.1, 0.01, 0.001, 0.0001, 0.00001)))
# 
# summary(output2)

#best model
model22 <- svm(newtrain,train_label,kernel = "radial", cost = 0.1, gamma = 0.1, cross = 10)
summary(model22)

#prediction
pred22 <- predict(model22,newtest)
SVM_rate <- mean(pred22 == test_label);SVM_rate
```

# 4th Method: ADAboosting and Bagging
```{r}
library(adabag)
library(ggplot2)
data4 <- cbind(train_label,newtrain)

#adaboosting
output4 <- boosting(train_label~., data = data4, boos = TRUE, mfinal = 100, coeflearn='Breiman')
pred4 <- predict.boosting(output4,newtest)
ADA_rate <- mean(pred4$class == test_label);ADA_rate

#check importance
# output4$importance
# importanceplot(output4,cex.names=.5)
# plot(output4$trees[[1]])
# text(output4$trees[[1]],pretty=0)

#bagging
output5 <- bagging(train_label~., data = data4, mfinal = 50)
pred5 <- predict.bagging(output5,newtest)
Bagging_rate <- mean(pred5$class == test_label);Bagging_rate
```

###ADAboosting with total 50 iterations for which the number of trees to use. Additionally, a bootstrap sample of the training set is drawn using the weights for each observation on that iteration.

# 6th Method: Extreme Gradient Boosting 
```{r}
require(xgboost)
require(methods)
require(data.table)
require(magrittr)
require(Ckmeans.1d.dp)

#convert to matrix form
dtrain_label <- as.matrix(as.numeric(train_label)) - 1
dtest_label <- as.matrix(as.numeric(test_label)) - 1 
dtrain <- xgb.DMatrix(data = as.matrix(newtrain), label= dtrain_label)
dtest <- xgb.DMatrix(data = as.matrix(newtest), label= dtest_label)

watchlist <- list(train = dtrain, test = dtest)
# nrounds:the max number of iterations
# nfold: cv size
# max.depth: the depth of tree.

# bst.cv <- xgb.cv(data = dtrain, booster = "gbtree", nfold = 10, nrounds = 5, eta = 0.001, max_depth = 2, nthread = 2, objective = "binary:logistic", "eval_metric" = "error")

#tree
bst1 <- xgb.train(data = dtrain, booster = "gbtree", max.depth = 6, eta = 0.001, nround = 5000, watchlist = watchlist,subsample = 0.8, objective = "binary:logistic", "eval_metric" = "error",early.stop.round = 100, params = list(set.seed(3)))

#linear
bst2 <- xgb.train(data = dtrain, booster = "gblinear", alpha = 0.001, nround = 5000, subsample = 0.8, watchlist = watchlist, objective = "binary:logistic", "eval_metric" = "error",early.stop.round = 100, params = list(set.seed(23)))

pred1 <- predict(bst1, dtest)
xgboost_tree <- mean((as.integer(pred1 > 0.5) == dtest_label));xgboost_tree

pred2 <- predict(bst2, dtest)
xgboost_linear <- mean((as.integer(pred2 > 0.5) == dtest_label));xgboost_linear

#importance plot
names <- dimnames(newtrain)[[2]]
importance_matrix <- xgb.importance(names, model = bst1)
xgb.plot.importance(importance_matrix)
```

# plot accuracy rate
```{r}
library(ggplot2)
AR <- c(0.804,0.8025,0.8055,0.797,0.81175,0.81375)
Method <- c("GLM","RandomForest","SVM_RBF","adaBoosting","xgBoosting_tree","xgBoosting_linear")
final <- data.frame(AccuracyRate = AR,Method = Method)
ggplot(final,aes(Method,AccuracyRate)) + geom_point(aes(color = Method,size = 3)) + ggtitle("AccuracyRate")
```

